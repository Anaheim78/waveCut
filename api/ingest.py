# api/ingest.py
# FastAPI â€” æ”¹è¿›ç‰ˆå˜Ÿå˜´æ£€æµ‹ï¼šæ›´åˆç†çš„é˜ˆå€¼ç­–ç•¥ + ç®€åŒ–æ®µè½å¤„ç† + ç›´è§‚è®¡æ•°
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse
import io, csv, math
import numpy as np
import pandas as pd

app = FastAPI()

# ===== æ”¹è¿›ç‰ˆæ ¸å¿ƒå‚æ•° =====
class DetectionConfig:
    # ä¿¡å·é¢„å¤„ç†
    SMOOTH_WINDOW_SEC = 0.1      # ç§»åŠ¨å¹³å‡çª—å£
    EXP_SMOOTH_ALPHA = 0.3       # æŒ‡æ•°å¹³æ»‘ç³»æ•°
    
    # é˜ˆå€¼ç­–ç•¥
    BASELINE_PERCENTILE = 30     # åŸºçº¿ç™¾åˆ†ä½æ•°
    ACTIVATION_PERCENTILE = 75   # æ¿€æ´»ç™¾åˆ†ä½æ•°
    NOISE_MULTIPLIER = 2.5       # å™ªå£°å€æ•°
    HYSTERESIS_RATIO = 0.7       # å›æ»æ¯”ä¾‹
    
    # æ®µè½å¤„ç†
    MIN_POUT_DURATION = 0.3      # æœ€å°å˜Ÿå˜´æŒç»­æ—¶é—´(ç§’)
    MAX_GAP_MERGE = 0.5          # æœ€å¤§åˆå¹¶é—´éš”(ç§’)
    PADDING_SEC = 0.1            # æ®µè½è¾¹ç¼˜æ‰©å±•(ç§’)

# ===== æ ¸å¿ƒå·¥å…·å‡½æ•° =====
def _estimate_fs(t: np.ndarray) -> float:
    """ä¼°ç®—é‡‡æ ·ç‡"""
    if len(t) < 2:
        return 20.0
    dt = float(np.median(np.diff(t)))
    return 1.0 / dt if dt > 1e-9 else 20.0

def _moving_average(x: np.ndarray, window_size: int) -> np.ndarray:
    """ç§»åŠ¨å¹³å‡æ»¤æ³¢"""
    if window_size <= 1:
        return x.copy()
    kernel = np.ones(int(window_size), dtype=float) / float(window_size)
    return np.convolve(x, kernel, mode="same")

def _exp_smooth(x: np.ndarray, alpha: float) -> np.ndarray:
    """æŒ‡æ•°å¹³æ»‘æ»¤æ³¢"""
    if alpha <= 0:
        return x.copy()
    result = np.zeros_like(x, dtype=float)
    result[0] = x[0]
    for i in range(1, len(x)):
        result[i] = alpha * x[i] + (1 - alpha) * result[i-1]
    return result

def _preprocess_signal(signal: np.ndarray, fs: float) -> np.ndarray:
    """ä¿¡å·é¢„å¤„ç†ï¼šç§»åŠ¨å¹³å‡ + æŒ‡æ•°å¹³æ»‘"""
    # ç§»åŠ¨å¹³å‡å»å™ª
    window_size = max(1, int(DetectionConfig.SMOOTH_WINDOW_SEC * fs))
    smoothed = _moving_average(signal, window_size)
    
    # æŒ‡æ•°å¹³æ»‘ç¨³å®šä¿¡å·
    processed = _exp_smooth(smoothed, DetectionConfig.EXP_SMOOTH_ALPHA)
    return processed

def _calculate_adaptive_thresholds(signal: np.ndarray) -> dict:
    """è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼"""
    # è®¡ç®—åŸºçº¿å’Œæ¿€æ´»æ°´å¹³
    baseline = np.percentile(signal, DetectionConfig.BASELINE_PERCENTILE)
    activation = np.percentile(signal, DetectionConfig.ACTIVATION_PERCENTILE)
    
    # ä¼°ç®—å™ªå£°æ°´å¹³ (MADæ–¹æ³•)
    median_val = np.median(signal)
    mad = np.median(np.abs(signal - median_val))
    noise_std = 1.4826 * mad  # MADåˆ°æ ‡å‡†å·®è½¬æ¢
    
    # åŠ¨æ€é˜ˆå€¼è®¡ç®—
    signal_range = activation - baseline
    if signal_range > DetectionConfig.NOISE_MULTIPLIER * noise_std:
        # ä¿¡å·è¶³å¤Ÿå¼ºï¼Œä½¿ç”¨ç™¾åˆ†ä½æ•°é˜ˆå€¼
        high_thresh = baseline + 0.6 * signal_range
    else:
        # ä¿¡å·è¾ƒå¼±ï¼Œä½¿ç”¨å™ªå£°åŸºç¡€é˜ˆå€¼
        high_thresh = baseline + DetectionConfig.NOISE_MULTIPLIER * noise_std
    
    # å›æ»é˜ˆå€¼
    low_thresh = baseline + DetectionConfig.HYSTERESIS_RATIO * (high_thresh - baseline)
    
    return {
        'high_threshold': high_thresh,
        'low_threshold': low_thresh,
        'baseline': baseline,
        'noise_std': noise_std
    }

def _detect_pout_segments(signal: np.ndarray, thresholds: dict) -> list:
    """ä½¿ç”¨å›æ»æ£€æµ‹å˜Ÿå˜´æ®µè½"""
    high_thresh = thresholds['high_threshold']
    low_thresh = thresholds['low_threshold']
    
    segments = []
    in_pout = False
    start_idx = 0
    
    for i, value in enumerate(signal):
        if not in_pout:
            # å¯»æ‰¾å˜Ÿå˜´å¼€å§‹
            if value >= high_thresh:
                in_pout = True
                start_idx = i
        else:
            # å¯»æ‰¾å˜Ÿå˜´ç»“æŸ
            if value < low_thresh:
                segments.append((start_idx, i))
                in_pout = False
    
    # å¤„ç†ç»“å°¾ä»åœ¨å˜Ÿå˜´çŠ¶æ€
    if in_pout:
        segments.append((start_idx, len(signal) - 1))
    
    return segments

def _merge_close_segments(segments: list, timestamps: np.ndarray, max_gap_sec: float) -> list:
    """åˆå¹¶ç›¸è¿‘çš„æ®µè½"""
    if len(segments) < 2:
        return segments
    
    merged = []
    current_start, current_end = segments[0]
    
    for start, end in segments[1:]:
        gap_time = timestamps[start] - timestamps[current_end]
        
        if gap_time <= max_gap_sec:
            # åˆå¹¶æ®µè½
            current_end = end
        else:
            # ä¿å­˜å½“å‰æ®µè½ï¼Œå¼€å§‹æ–°æ®µè½
            merged.append((current_start, current_end))
            current_start, current_end = start, end
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
    merged.append((current_start, current_end))
    return merged

def _filter_by_duration(segments: list, timestamps: np.ndarray, min_duration_sec: float) -> list:
    """è¿‡æ»¤å¤ªçŸ­çš„æ®µè½"""
    filtered = []
    for start, end in segments:
        duration = timestamps[end] - timestamps[start]
        if duration >= min_duration_sec:
            filtered.append((start, end))
    return filtered

def _apply_padding(segments: list, signal_length: int, fs: float, padding_sec: float) -> list:
    """ä¸ºæ®µè½æ·»åŠ è¾¹ç¼˜æ‰©å±•"""
    padding_samples = int(padding_sec * fs)
    padded = []
    
    for start, end in segments:
        new_start = max(0, start - padding_samples)
        new_end = min(signal_length - 1, end + padding_samples)
        padded.append((new_start, new_end))
    
    return padded

def _segments_to_results(segments: list, timestamps: np.ndarray) -> list:
    """å°†æ®µè½ç´¢å¼•è½¬æ¢ä¸ºç»“æœæ ¼å¼"""
    results = []
    for i, (start, end) in enumerate(segments):
        start_time = timestamps[start]
        end_time = timestamps[end]
        duration = end_time - start_time
        
        results.append({
            "index": i,
            "start_time": round(float(start_time), 3),
            "end_time": round(float(end_time), 3),
            "duration": round(float(duration), 3)
        })
    
    return results

# ===== ä¸»åˆ†æå‡½æ•° =====
def analyze_improved_pout(t: np.ndarray, r: np.ndarray) -> dict:
    """æ”¹è¿›ç‰ˆå˜Ÿå˜´åˆ†æç®—æ³•"""
    # ä¼°ç®—é‡‡æ ·ç‡
    fs = _estimate_fs(t)
    
    # ä¿¡å·é¢„å¤„ç†
    processed_signal = _preprocess_signal(r, fs)
    
    # è®¡ç®—è‡ªé€‚åº”é˜ˆå€¼
    thresholds = _calculate_adaptive_thresholds(processed_signal)
    
    # æ£€æµ‹åˆæ­¥æ®µè½
    raw_segments = _detect_pout_segments(processed_signal, thresholds)
    
    # å¦‚æœå®Œå…¨æ£€æµ‹ä¸åˆ°ï¼Œæ”¾å®½é˜ˆå€¼å†è¯•ä¸€æ¬¡
    if len(raw_segments) == 0:
        # é™ä½é«˜é˜ˆå€¼15%
        relaxed_high = thresholds['baseline'] + 0.85 * (thresholds['high_threshold'] - thresholds['baseline'])
        relaxed_low = thresholds['baseline'] + DetectionConfig.HYSTERESIS_RATIO * (relaxed_high - thresholds['baseline'])
        relaxed_thresholds = {
            'high_threshold': relaxed_high,
            'low_threshold': relaxed_low,
            'baseline': thresholds['baseline'],
            'noise_std': thresholds['noise_std']
        }
        raw_segments = _detect_pout_segments(processed_signal, relaxed_thresholds)
        thresholds = relaxed_thresholds  # æ›´æ–°ç”¨äºè¾“å‡ºçš„é˜ˆå€¼
    
    # æ®µè½åå¤„ç†æµç¨‹
    # 1. åˆå¹¶ç›¸è¿‘æ®µè½
    merged_segments = _merge_close_segments(raw_segments, t, DetectionConfig.MAX_GAP_MERGE)
    
    # 2. è¿‡æ»¤çŸ­æ®µè½
    filtered_segments = _filter_by_duration(merged_segments, t, DetectionConfig.MIN_POUT_DURATION)
    
    # 3. æ·»åŠ è¾¹ç¼˜æ‰©å±•
    final_segments = _apply_padding(filtered_segments, len(processed_signal), fs, DetectionConfig.PADDING_SEC)
    
    # è½¬æ¢ä¸ºè¾“å‡ºæ ¼å¼
    segments = _segments_to_results(final_segments, t)
    
    # è®¡ç®—æŒ‡æ ‡
    pout_count = len(segments)
    total_hold_time = sum(seg["duration"] for seg in segments)
    breakpoints = [round(seg["end_time"], 2) for seg in segments]
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼ˆç”¨äºæ—¥å¿—ï¼‰
    print(f"ğŸ” æ”¹è¿›ç‰ˆæ£€æµ‹ç»“æœ: {pout_count}æ¬¡å˜Ÿå˜´, æ€»æ—¶é•¿{total_hold_time:.2f}s")
    print(f"ğŸ“Š é˜ˆå€¼: é«˜={thresholds['high_threshold']:.3f}, ä½={thresholds['low_threshold']:.3f}")
    print(f"ğŸ¯ åŸå§‹æ®µè½: {len(raw_segments)}, æœ€ç»ˆæ®µè½: {len(final_segments)}")
    
    return {
        "motion": "poutLip",
        "pout_count": pout_count,
        "total_hold_time": round(float(total_hold_time), 3),
        "breakpoints": breakpoints,
        "segments": segments
    }

# ===== FastAPIè·¯ç”± =====
@app.post("/")
async def ingest(req: Request):
    try:
        body = await req.json()
        lines = body.get("lines") or []
        
        if not lines or not isinstance(lines, list):
            return JSONResponse({"message": "no lines", "receivedCount": 0}, status_code=400)

        header = lines[0]
        cols_lower = [c.strip().lower() for c in header.split(",")]

        # æ£€æŸ¥æ˜¯å¦æ˜¯å˜Ÿå˜´æ•°æ®
        if "height_width_ratio" in cols_lower:
            try:
                # è½¬æ¢ä¸ºDataFrame
                reader = csv.DictReader(io.StringIO("\n".join(lines)))
                df = pd.DataFrame(reader)

                # å»ºç«‹åˆ—åæ˜ å°„ï¼ˆå®¹é”™å¤§å°å†™å’Œç©ºç™½ï¼‰
                lowmap = {c.strip().lower(): c for c in df.columns}
                
                if "time_seconds" not in lowmap or "height_width_ratio" not in lowmap:
                    return JSONResponse({
                        "message": "IMPROVED API OK", 
                        "motion": "poutLip",
                        "error": "missing required columns"
                    }, status_code=400)

                # æå–å¹¶æ¸…ç†æ•°æ®
                t = pd.to_numeric(df[lowmap["time_seconds"]], errors="coerce").to_numpy()
                r = pd.to_numeric(df[lowmap["height_width_ratio"]], errors="coerce").to_numpy()
                
                # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
                valid_mask = np.isfinite(t) & np.isfinite(r)
                t, r = t[valid_mask], r[valid_mask]

                if len(t) < 2:
                    return JSONResponse({
                        "message": "IMPROVED API OK",
                        "motion": "poutLip",
                        "pout_count": 0,
                        "total_hold_time": 0.0,
                        "breakpoints": [],
                        "segments": []
                    }, status_code=200)

                # ä½¿ç”¨æ”¹è¿›ç‰ˆç®—æ³•åˆ†æ
                result_core = analyze_improved_pout(t, r)
                result = {"message": "IMPROVED API OK", **result_core}

                # è¾“å‡ºåˆ°Railwayæ—¥å¿—ä¾¿äºè°ƒè¯•
                print("âœ… æ”¹è¿›ç‰ˆAPIç»“æœ:", result)
                
                return JSONResponse(result, status_code=200)
                
            except Exception as e:
                print(f"âŒ å˜Ÿå˜´åˆ†æé”™è¯¯: {str(e)}")
                return JSONResponse({
                    "message": "IMPROVED API ERROR",
                    "motion": "poutLip",
                    "error": str(e),
                    "pout_count": 0,
                    "total_hold_time": 0.0,
                    "breakpoints": [],
                    "segments": []
                }, status_code=500)

        # éå˜Ÿå˜´æ•°æ®
        return JSONResponse({
            "message": "IMPROVED API OK",
            "motion": "unknown",
            "reason": "missing height_width_ratio in header",
            "receivedCount": max(0, len(lines) - 1)
        }, status_code=200)
        
    except Exception as e:
        print(f"âŒ APIæ€»ä½“é”™è¯¯: {str(e)}")
        return JSONResponse({
            "message": "IMPROVED API ERROR",
            "error": str(e)
        }, status_code=500)

# ===== å¥åº·æ£€æŸ¥è·¯ç”± =====
@app.get("/health")
async def health_check():
    return {"status": "healthy", "version": "improved_v1.0"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)